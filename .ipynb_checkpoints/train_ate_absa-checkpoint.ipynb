{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ae_dataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens, tags, pols = self.df.iloc[idx, :3].values\n",
    "\n",
    "        tokens = tokens.replace(\"'\", \"\").strip(\"][\").split(', ')\n",
    "        tags = tags.strip('][').split(', ')\n",
    "        pols = pols.strip('][').split(', ')\n",
    "\n",
    "        bert_tokens = []\n",
    "        bert_tags = []\n",
    "        bert_pols = []\n",
    "        for i in range(len(tokens)):\n",
    "            t = tokenizer.tokenize(tokens[i])\n",
    "            bert_tokens += t\n",
    "            bert_tags += [int(tags[i])]*len(t)\n",
    "            bert_pols += [int(pols[i])]*len(t)\n",
    "        \n",
    "        bert_ids = tokenizer.convert_tokens_to_ids(bert_tokens)\n",
    "\n",
    "        ids_tensor = torch.tensor(bert_ids)\n",
    "        tags_tensor = torch.tensor(bert_tags)\n",
    "        pols_tensor = torch.tensor(bert_pols)\n",
    "\n",
    "        return bert_tokens, ids_tensor, tags_tensor, pols_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"laptops_train.csv\")\n",
    "laptops_train_ds = ae_dataset(df, tokenizer)\n",
    "\n",
    "df = pd.read_csv(\"laptops_test.csv\")\n",
    "laptops_test_ds = ae_dataset(df, tokenizer)\n",
    "\n",
    "df = pd.read_csv(\"restaurants_train.csv\")\n",
    "restaurants_train_ds = ae_dataset(df, tokenizer)\n",
    "\n",
    "df = pd.read_csv(\"restaurants_test.csv\")\n",
    "restaurants_test_ds = ae_dataset(df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'battery', 'life', 'seems', 'to', 'be', 'very', 'good', ',', 'and', 'have', 'had', 'no', 'issues', 'with', 'it', '.']\n",
      "17\n",
      "tensor([1996, 6046, 2166, 3849, 2000, 2022, 2200, 2204, 1010, 1998, 2031, 2018,\n",
      "        2053, 3314, 2007, 2009, 1012])\n",
      "17\n",
      "tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "17\n",
      "tensor([0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "w,x,y,z = laptops_train_ds.__getitem__(121)\n",
    "print(w)\n",
    "print(len(w))\n",
    "print(x)\n",
    "print(len(x))\n",
    "print(y)\n",
    "print(len(y))\n",
    "print(z)\n",
    "print(len(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ConcatDataset([laptops_train_ds, restaurants_train_ds])\n",
    "test_ds = ConcatDataset([laptops_test_ds, restaurants_test_ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True)\n",
    "\n",
    "    pols_tensors = [s[3] for s in samples]\n",
    "    pols_tensors = pad_sequence(pols_tensors, batch_first=True)\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1)\n",
    "    \n",
    "    return ids_tensors, tags_tensors, pols_tensors, masks_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=5, collate_fn=create_mini_batch, shuffle = True)\n",
    "test_loader = DataLoader(test_ds, batch_size=50, collate_fn=create_mini_batch, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1045,  3641,  1037, 27333,  2072,  1998,  2356,  1018,  2335,  2005,\n",
      "          2009,  2021,  2196,  2288,  2009,  1012,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0],\n",
      "        [ 2204,  2189,  1010,  2307,  2833,  1010, 26203,  2326, 15184,  7597,\n",
      "          1012,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0],\n",
      "        [ 1996,  4524,  9050,  2031,  2019,  5151,  5510,  2007,  1037, 27547,\n",
      "         14902,  1010,  2119, 21271,  2100,  2664,  2025, 16031,  8029,  1012,\n",
      "             0,     0,     0,     0,     0],\n",
      "        [ 3524,  3095,  2003,  1038,  5802,  6528, 14626, 14477,  9397,  2890,\n",
      "          7405,  6024,  1997,  2115,  2449,  2021,  2049,  1996,  2190, 11345,\n",
      "          2006,  1996,  1057,  9333,   999],\n",
      "        [ 1996,  6046,  2515,  1000, 23961,  1000,  2197,  2146,  2021,  1045,\n",
      "          1000,  1049,  1000,  2469,  2019, 12200,  6046,  2052,  9611,  2008,\n",
      "          3291,  1012,     0,     0,     0]])\n",
      "torch.Size([5, 25])\n",
      "tensor([[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0]])\n",
      "torch.Size([5, 25])\n",
      "tensor([[0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0]])\n",
      "torch.Size([5, 25])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0]])\n",
      "torch.Size([5, 25])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    w,x,y,z = batch\n",
    "    print(w)\n",
    "    print(w.size())\n",
    "    print(x)\n",
    "    print(x.size())\n",
    "    print(y)\n",
    "    print(y.size())\n",
    "    print(z)\n",
    "    print(z.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bert aspect extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bert_aspect_extraction(torch.nn.Module):\n",
    "    def __init__(self, pretrain_model):\n",
    "        super(bert_aspect_extraction, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(pretrain_model)\n",
    "        self.linear = torch.nn.Linear(self.bert.config.hidden_size, 2)\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, ids_tensors, tags_tensors, masks_tensors):\n",
    "        bert_outputs,_ = self.bert(input_ids=ids_tensors, attention_mask=masks_tensors)\n",
    "        # print(bert_outputs.size())\n",
    "        linear_outputs = self.linear(bert_outputs)\n",
    "        # print(linear_outputs.size())\n",
    "\n",
    "        if tags_tensors is not None:\n",
    "            tags_tensors = tags_tensors.view(-1)\n",
    "            linear_outputs = linear_outputs.view(-1,2)\n",
    "            # print(linear_outputs.size())\n",
    "            # print(tags_tensors.size())\n",
    "            loss = self.loss_fn(linear_outputs, tags_tensors)\n",
    "            return loss\n",
    "        else:\n",
    "            return linear_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "lr = 2e-5\n",
    "model = bert_aspect_extraction(\"bert-base-uncased\").to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evl_time(t):\n",
    "    min, sec= divmod(t, 60)\n",
    "    hr, min = divmod(min, 60)\n",
    "    return int(hr), int(min), int(sec)\n",
    "\n",
    "def load_model(path):\n",
    "    model.load_state_dict(torch.load(path), strict=False)\n",
    "    \n",
    "def save_model(name):\n",
    "    torch.save(model.state_dict(), name)\n",
    "\n",
    "def train(loader, epochs):\n",
    "    all_data = len(loader)\n",
    "    for epoch in range(epochs):\n",
    "        finish_data = 0\n",
    "        losses = []\n",
    "        current_times = []\n",
    "        correct_predictions = 0\n",
    "        \n",
    "        for data in loader:\n",
    "            t0 = time.time()\n",
    "            ids_tensors, tags_tensors, _, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(DEVICE)\n",
    "            tags_tensors = tags_tensors.to(DEVICE)\n",
    "            masks_tensors = masks_tensors.to(DEVICE)\n",
    "\n",
    "            loss = model(ids_tensors=ids_tensors, tags_tensors=tags_tensors, masks_tensors=masks_tensors)\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            finish_data += 1\n",
    "            current_times.append(round(time.time()-t0,3))\n",
    "            current = np.mean(current_times)\n",
    "            hr, min, sec = evl_time(current*(all_data-finish_data) + current*all_data*(epochs-epoch-1))\n",
    "            print('epoch:', epoch, \" batch:\", finish_data, \"/\" , all_data, \" loss:\", np.mean(losses), \" hr:\", hr, \" min:\", min,\" sec:\", sec)         \n",
    "\n",
    "        save_model('train-epoch-temp.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  loss: 0.006587694087148549  hr: 0  min: 0  sec: 16\n",
      "epoch: 2  batch: 922 / 1170  loss: 0.00658561877199939  hr: 0  min: 0  sec: 16\n",
      "epoch: 2  batch: 923 / 1170  loss: 0.006582272819261408  hr: 0  min: 0  sec: 16\n",
      "epoch: 2  batch: 924 / 1170  loss: 0.0066696877986890946  hr: 0  min: 0  sec: 15\n",
      "epoch: 2  batch: 925 / 1170  loss: 0.00666298043509832  hr: 0  min: 0  sec: 15\n",
      "epoch: 2  batch: 926 / 1170  loss: 0.006655934066853645  hr: 0  min: 0  sec: 15\n",
      "epoch: 2  batch: 927 / 1170  loss: 0.006675612405716103  hr: 0  min: 0  sec: 15\n",
      "epoch: 2  batch: 928 / 1170  loss: 0.006668631931586725  hr: 0  min: 0  sec: 15\n",
      "epoch: 2  batch: 929 / 1170  loss: 0.0066617088813051  hr: 0  min: 0  sec: 15\n",
      "epoch: 2  batch: 930 / 1170  loss: 0.006655523707208935  hr: 0  min: 0  sec: 15\n",
      "epoch: 2  batch: 931 / 1170  loss: 0.006648549227490766  hr: 0  min: 0  sec: 15\n",
      "epoch: 2  batch: 932 / 1170  loss: 0.0066438672362029595  hr: 0  min: 0  sec: 15\n",
      "epoch: 2  batch: 933 / 1170  loss: 0.0066370976475257845  hr: 0  min: 0  sec: 15\n",
      "epoch: 2  batch: 934 / 1170  loss: 0.006634389985351134  hr: 0  min: 0  sec: 15\n",
      "epoch: 2  batch: 935 / 1170  loss: 0.006634326576224757  hr: 0  min: 0  sec: 15\n",
      "epoch: 2  batch: 936 / 1170  loss: 0.006631700928719355  hr: 0  min: 0  sec: 15\n",
      "epoch: 2  batch: 937 / 1170  loss: 0.0066255223160039825  hr: 0  min: 0  sec: 15\n",
      "epoch: 2  batch: 938 / 1170  loss: 0.006620661566092415  hr: 0  min: 0  sec: 15\n",
      "epoch: 2  batch: 939 / 1170  loss: 0.0066143032684554005  hr: 0  min: 0  sec: 15\n",
      "epoch: 2  batch: 940 / 1170  loss: 0.006614039325637699  hr: 0  min: 0  sec: 14\n",
      "epoch: 2  batch: 941 / 1170  loss: 0.006613451457558537  hr: 0  min: 0  sec: 14\n",
      "epoch: 2  batch: 942 / 1170  loss: 0.006606988526947847  hr: 0  min: 0  sec: 14\n",
      "epoch: 2  batch: 943 / 1170  loss: 0.006600420704132567  hr: 0  min: 0  sec: 14\n",
      "epoch: 2  batch: 944 / 1170  loss: 0.006594039049313154  hr: 0  min: 0  sec: 14\n",
      "epoch: 2  batch: 945 / 1170  loss: 0.006587428102875252  hr: 0  min: 0  sec: 14\n",
      "epoch: 2  batch: 946 / 1170  loss: 0.006580709983851543  hr: 0  min: 0  sec: 14\n",
      "epoch: 2  batch: 947 / 1170  loss: 0.006573963806802926  hr: 0  min: 0  sec: 14\n",
      "epoch: 2  batch: 948 / 1170  loss: 0.006567605708096764  hr: 0  min: 0  sec: 14\n",
      "epoch: 2  batch: 949 / 1170  loss: 0.006561244323240491  hr: 0  min: 0  sec: 14\n",
      "epoch: 2  batch: 950 / 1170  loss: 0.00655485399499364  hr: 0  min: 0  sec: 14\n",
      "epoch: 2  batch: 951 / 1170  loss: 0.006548682661839936  hr: 0  min: 0  sec: 14\n",
      "epoch: 2  batch: 952 / 1170  loss: 0.006542422818703559  hr: 0  min: 0  sec: 14\n",
      "epoch: 2  batch: 953 / 1170  loss: 0.006561062356586845  hr: 0  min: 0  sec: 14\n",
      "epoch: 2  batch: 954 / 1170  loss: 0.006554506775672671  hr: 0  min: 0  sec: 14\n",
      "epoch: 2  batch: 955 / 1170  loss: 0.006548084963615301  hr: 0  min: 0  sec: 14\n",
      "epoch: 2  batch: 956 / 1170  loss: 0.006541448251600475  hr: 0  min: 0  sec: 13\n",
      "epoch: 2  batch: 957 / 1170  loss: 0.006534784791869226  hr: 0  min: 0  sec: 13\n",
      "epoch: 2  batch: 958 / 1170  loss: 0.006594265491276589  hr: 0  min: 0  sec: 13\n",
      "epoch: 2  batch: 959 / 1170  loss: 0.006587805980534853  hr: 0  min: 0  sec: 13\n",
      "epoch: 2  batch: 960 / 1170  loss: 0.006597415251599159  hr: 0  min: 0  sec: 13\n",
      "epoch: 2  batch: 961 / 1170  loss: 0.006590689667423631  hr: 0  min: 0  sec: 13\n",
      "epoch: 2  batch: 962 / 1170  loss: 0.0065845317930969885  hr: 0  min: 0  sec: 13\n",
      "epoch: 2  batch: 963 / 1170  loss: 0.0065778269207663305  hr: 0  min: 0  sec: 13\n",
      "epoch: 2  batch: 964 / 1170  loss: 0.006571876616785385  hr: 0  min: 0  sec: 13\n",
      "epoch: 2  batch: 965 / 1170  loss: 0.006565715415102256  hr: 0  min: 0  sec: 13\n",
      "epoch: 2  batch: 966 / 1170  loss: 0.006559540036917875  hr: 0  min: 0  sec: 13\n",
      "epoch: 2  batch: 967 / 1170  loss: 0.006580751265400723  hr: 0  min: 0  sec: 13\n",
      "epoch: 2  batch: 968 / 1170  loss: 0.006574286228844493  hr: 0  min: 0  sec: 13\n",
      "epoch: 2  batch: 969 / 1170  loss: 0.00656874082178866  hr: 0  min: 0  sec: 13\n",
      "epoch: 2  batch: 970 / 1170  loss: 0.006562562661498099  hr: 0  min: 0  sec: 13\n",
      "epoch: 2  batch: 971 / 1170  loss: 0.006556234559590111  hr: 0  min: 0  sec: 12\n",
      "epoch: 2  batch: 972 / 1170  loss: 0.006551248438893113  hr: 0  min: 0  sec: 12\n",
      "epoch: 2  batch: 973 / 1170  loss: 0.006545433630731425  hr: 0  min: 0  sec: 12\n",
      "epoch: 2  batch: 974 / 1170  loss: 0.006545623594389304  hr: 0  min: 0  sec: 12\n",
      "epoch: 2  batch: 975 / 1170  loss: 0.00653905486046241  hr: 0  min: 0  sec: 12\n",
      "epoch: 2  batch: 976 / 1170  loss: 0.0066479703401738765  hr: 0  min: 0  sec: 12\n",
      "epoch: 2  batch: 977 / 1170  loss: 0.006641375874962154  hr: 0  min: 0  sec: 12\n",
      "epoch: 2  batch: 978 / 1170  loss: 0.006635199701479576  hr: 0  min: 0  sec: 12\n",
      "epoch: 2  batch: 979 / 1170  loss: 0.006628791189217294  hr: 0  min: 0  sec: 12\n",
      "epoch: 2  batch: 980 / 1170  loss: 0.006624252649189842  hr: 0  min: 0  sec: 12\n",
      "epoch: 2  batch: 981 / 1170  loss: 0.00661771940216638  hr: 0  min: 0  sec: 12\n",
      "epoch: 2  batch: 982 / 1170  loss: 0.006611213011230516  hr: 0  min: 0  sec: 12\n",
      "epoch: 2  batch: 983 / 1170  loss: 0.006638684590472764  hr: 0  min: 0  sec: 12\n",
      "epoch: 2  batch: 984 / 1170  loss: 0.006633162366811823  hr: 0  min: 0  sec: 12\n",
      "epoch: 2  batch: 985 / 1170  loss: 0.006626580191405637  hr: 0  min: 0  sec: 12\n",
      "epoch: 2  batch: 986 / 1170  loss: 0.0066208288158224205  hr: 0  min: 0  sec: 12\n",
      "epoch: 2  batch: 987 / 1170  loss: 0.006614652304208211  hr: 0  min: 0  sec: 11\n",
      "epoch: 2  batch: 988 / 1170  loss: 0.006634405504411717  hr: 0  min: 0  sec: 11\n",
      "epoch: 2  batch: 989 / 1170  loss: 0.006628500009408194  hr: 0  min: 0  sec: 11\n",
      "epoch: 2  batch: 990 / 1170  loss: 0.006630640347959177  hr: 0  min: 0  sec: 11\n",
      "epoch: 2  batch: 991 / 1170  loss: 0.00662611199717738  hr: 0  min: 0  sec: 11\n",
      "epoch: 2  batch: 992 / 1170  loss: 0.006622717178449802  hr: 0  min: 0  sec: 11\n",
      "epoch: 2  batch: 993 / 1170  loss: 0.006648060245419363  hr: 0  min: 0  sec: 11\n",
      "epoch: 2  batch: 994 / 1170  loss: 0.006642822888778767  hr: 0  min: 0  sec: 11\n",
      "epoch: 2  batch: 995 / 1170  loss: 0.006639934396479471  hr: 0  min: 0  sec: 11\n",
      "epoch: 2  batch: 996 / 1170  loss: 0.00663933869341473  hr: 0  min: 0  sec: 11\n",
      "epoch: 2  batch: 997 / 1170  loss: 0.0066389048235392135  hr: 0  min: 0  sec: 11\n",
      "epoch: 2  batch: 998 / 1170  loss: 0.006635073254427678  hr: 0  min: 0  sec: 11\n",
      "epoch: 2  batch: 999 / 1170  loss: 0.006728573406066347  hr: 0  min: 0  sec: 11\n",
      "epoch: 2  batch: 1000 / 1170  loss: 0.006723397050300264  hr: 0  min: 0  sec: 11\n",
      "epoch: 2  batch: 1001 / 1170  loss: 0.006717479200994751  hr: 0  min: 0  sec: 11\n",
      "epoch: 2  batch: 1002 / 1170  loss: 0.006719168750855616  hr: 0  min: 0  sec: 10\n",
      "epoch: 2  batch: 1003 / 1170  loss: 0.006713090801430129  hr: 0  min: 0  sec: 10\n",
      "epoch: 2  batch: 1004 / 1170  loss: 0.006708592587963801  hr: 0  min: 0  sec: 10\n",
      "epoch: 2  batch: 1005 / 1170  loss: 0.006705877467175015  hr: 0  min: 0  sec: 10\n",
      "epoch: 2  batch: 1006 / 1170  loss: 0.0067074927414734345  hr: 0  min: 0  sec: 10\n",
      "epoch: 2  batch: 1007 / 1170  loss: 0.006701870943626454  hr: 0  min: 0  sec: 10\n",
      "epoch: 2  batch: 1008 / 1170  loss: 0.006695800267094917  hr: 0  min: 0  sec: 10\n",
      "epoch: 2  batch: 1009 / 1170  loss: 0.00668933463350494  hr: 0  min: 0  sec: 10\n",
      "epoch: 2  batch: 1010 / 1170  loss: 0.0066858666414731545  hr: 0  min: 0  sec: 10\n",
      "epoch: 2  batch: 1011 / 1170  loss: 0.006679620694282256  hr: 0  min: 0  sec: 10\n",
      "epoch: 2  batch: 1012 / 1170  loss: 0.006673308968673536  hr: 0  min: 0  sec: 10\n",
      "epoch: 2  batch: 1013 / 1170  loss: 0.006666993694652289  hr: 0  min: 0  sec: 10\n",
      "epoch: 2  batch: 1014 / 1170  loss: 0.00666070800132593  hr: 0  min: 0  sec: 10\n",
      "epoch: 2  batch: 1015 / 1170  loss: 0.006656532982918273  hr: 0  min: 0  sec: 10\n",
      "epoch: 2  batch: 1016 / 1170  loss: 0.00665027278067666  hr: 0  min: 0  sec: 10\n",
      "epoch: 2  batch: 1017 / 1170  loss: 0.006645490374442899  hr: 0  min: 0  sec: 9\n",
      "epoch: 2  batch: 1018 / 1170  loss: 0.006639308919703582  hr: 0  min: 0  sec: 9\n",
      "epoch: 2  batch: 1019 / 1170  loss: 0.006639219971388072  hr: 0  min: 0  sec: 9\n",
      "epoch: 2  batch: 1020 / 1170  loss: 0.006642008676991442  hr: 0  min: 0  sec: 9\n",
      "epoch: 2  batch: 1021 / 1170  loss: 0.006635711276331934  hr: 0  min: 0  sec: 9\n",
      "epoch: 2  batch: 1022 / 1170  loss: 0.006629382782261349  hr: 0  min: 0  sec: 9\n",
      "epoch: 2  batch: 1023 / 1170  loss: 0.006623972469341863  hr: 0  min: 0  sec: 9\n",
      "epoch: 2  batch: 1024 / 1170  loss: 0.006617925928466661  hr: 0  min: 0  sec: 9\n",
      "epoch: 2  batch: 1025 / 1170  loss: 0.006611828225701707  hr: 0  min: 0  sec: 9\n",
      "epoch: 2  batch: 1026 / 1170  loss: 0.006605672834661302  hr: 0  min: 0  sec: 9\n",
      "epoch: 2  batch: 1027 / 1170  loss: 0.006599385303282723  hr: 0  min: 0  sec: 9\n",
      "epoch: 2  batch: 1028 / 1170  loss: 0.006700998492122076  hr: 0  min: 0  sec: 9\n",
      "epoch: 2  batch: 1029 / 1170  loss: 0.006711793709719989  hr: 0  min: 0  sec: 9\n",
      "epoch: 2  batch: 1030 / 1170  loss: 0.0067771515069112356  hr: 0  min: 0  sec: 9\n",
      "epoch: 2  batch: 1031 / 1170  loss: 0.006773055305184049  hr: 0  min: 0  sec: 9\n",
      "epoch: 2  batch: 1032 / 1170  loss: 0.0067695564194418835  hr: 0  min: 0  sec: 9\n",
      "epoch: 2  batch: 1033 / 1170  loss: 0.006810069366288687  hr: 0  min: 0  sec: 8\n",
      "epoch: 2  batch: 1034 / 1170  loss: 0.006803923269820591  hr: 0  min: 0  sec: 8\n",
      "epoch: 2  batch: 1035 / 1170  loss: 0.006797933140204396  hr: 0  min: 0  sec: 8\n",
      "epoch: 2  batch: 1036 / 1170  loss: 0.006812187640688496  hr: 0  min: 0  sec: 8\n",
      "epoch: 2  batch: 1037 / 1170  loss: 0.00680575846715908  hr: 0  min: 0  sec: 8\n",
      "epoch: 2  batch: 1038 / 1170  loss: 0.006799431496523213  hr: 0  min: 0  sec: 8\n",
      "epoch: 2  batch: 1039 / 1170  loss: 0.006793193940602779  hr: 0  min: 0  sec: 8\n",
      "epoch: 2  batch: 1040 / 1170  loss: 0.006787384953825564  hr: 0  min: 0  sec: 8\n",
      "epoch: 2  batch: 1041 / 1170  loss: 0.006781742228495894  hr: 0  min: 0  sec: 8\n",
      "epoch: 2  batch: 1042 / 1170  loss: 0.0067791770999831545  hr: 0  min: 0  sec: 8\n",
      "epoch: 2  batch: 1043 / 1170  loss: 0.006781992854239966  hr: 0  min: 0  sec: 8\n",
      "epoch: 2  batch: 1044 / 1170  loss: 0.006775881033533492  hr: 0  min: 0  sec: 8\n",
      "epoch: 2  batch: 1045 / 1170  loss: 0.0067699312949308045  hr: 0  min: 0  sec: 8\n",
      "epoch: 2  batch: 1046 / 1170  loss: 0.006764201118767059  hr: 0  min: 0  sec: 8\n",
      "epoch: 2  batch: 1047 / 1170  loss: 0.006766941101228464  hr: 0  min: 0  sec: 8\n",
      "epoch: 2  batch: 1048 / 1170  loss: 0.00680745729125942  hr: 0  min: 0  sec: 7\n",
      "epoch: 2  batch: 1049 / 1170  loss: 0.006806412063575872  hr: 0  min: 0  sec: 7\n",
      "epoch: 2  batch: 1050 / 1170  loss: 0.0068006653522163455  hr: 0  min: 0  sec: 7\n",
      "epoch: 2  batch: 1051 / 1170  loss: 0.0068133988775570765  hr: 0  min: 0  sec: 7\n",
      "epoch: 2  batch: 1052 / 1170  loss: 0.006807403239251571  hr: 0  min: 0  sec: 7\n",
      "epoch: 2  batch: 1053 / 1170  loss: 0.006801430464426811  hr: 0  min: 0  sec: 7\n",
      "epoch: 2  batch: 1054 / 1170  loss: 0.006813746492976616  hr: 0  min: 0  sec: 7\n",
      "epoch: 2  batch: 1055 / 1170  loss: 0.0068098674204986175  hr: 0  min: 0  sec: 7\n",
      "epoch: 2  batch: 1056 / 1170  loss: 0.006807089698916322  hr: 0  min: 0  sec: 7\n",
      "epoch: 2  batch: 1057 / 1170  loss: 0.00680303017997827  hr: 0  min: 0  sec: 7\n",
      "epoch: 2  batch: 1058 / 1170  loss: 0.0068037569359348056  hr: 0  min: 0  sec: 7\n",
      "epoch: 2  batch: 1059 / 1170  loss: 0.006797581107145249  hr: 0  min: 0  sec: 7\n",
      "epoch: 2  batch: 1060 / 1170  loss: 0.0067917004496985955  hr: 0  min: 0  sec: 7\n",
      "epoch: 2  batch: 1061 / 1170  loss: 0.006847599363855317  hr: 0  min: 0  sec: 7\n",
      "epoch: 2  batch: 1062 / 1170  loss: 0.00684167383968845  hr: 0  min: 0  sec: 7\n",
      "epoch: 2  batch: 1063 / 1170  loss: 0.006839582183064721  hr: 0  min: 0  sec: 6\n",
      "epoch: 2  batch: 1064 / 1170  loss: 0.006842422369029242  hr: 0  min: 0  sec: 6\n",
      "epoch: 2  batch: 1065 / 1170  loss: 0.006845825801176093  hr: 0  min: 0  sec: 6\n",
      "epoch: 2  batch: 1066 / 1170  loss: 0.00684570014253665  hr: 0  min: 0  sec: 6\n",
      "epoch: 2  batch: 1067 / 1170  loss: 0.0068472716053120785  hr: 0  min: 0  sec: 6\n",
      "epoch: 2  batch: 1068 / 1170  loss: 0.006841325565137288  hr: 0  min: 0  sec: 6\n",
      "epoch: 2  batch: 1069 / 1170  loss: 0.006835220524033016  hr: 0  min: 0  sec: 6\n",
      "epoch: 2  batch: 1070 / 1170  loss: 0.006829050884236121  hr: 0  min: 0  sec: 6\n",
      "epoch: 2  batch: 1071 / 1170  loss: 0.006822807436973512  hr: 0  min: 0  sec: 6\n",
      "epoch: 2  batch: 1072 / 1170  loss: 0.006816564993587506  hr: 0  min: 0  sec: 6\n",
      "epoch: 2  batch: 1073 / 1170  loss: 0.00681053990591925  hr: 0  min: 0  sec: 6\n",
      "epoch: 2  batch: 1074 / 1170  loss: 0.006805054223682173  hr: 0  min: 0  sec: 6\n",
      "epoch: 2  batch: 1075 / 1170  loss: 0.006798827356813194  hr: 0  min: 0  sec: 6\n",
      "epoch: 2  batch: 1076 / 1170  loss: 0.0067928835294973474  hr: 0  min: 0  sec: 6\n",
      "epoch: 2  batch: 1077 / 1170  loss: 0.006793819369331323  hr: 0  min: 0  sec: 6\n",
      "epoch: 2  batch: 1078 / 1170  loss: 0.006787733091693177  hr: 0  min: 0  sec: 6\n",
      "epoch: 2  batch: 1079 / 1170  loss: 0.006859993275312256  hr: 0  min: 0  sec: 5\n",
      "epoch: 2  batch: 1080 / 1170  loss: 0.006859141090414292  hr: 0  min: 0  sec: 5\n",
      "epoch: 2  batch: 1081 / 1170  loss: 0.0068530541081585645  hr: 0  min: 0  sec: 5\n",
      "epoch: 2  batch: 1082 / 1170  loss: 0.006846926624445777  hr: 0  min: 0  sec: 5\n",
      "epoch: 2  batch: 1083 / 1170  loss: 0.0068425564010189425  hr: 0  min: 0  sec: 5\n",
      "epoch: 2  batch: 1084 / 1170  loss: 0.006836541135969608  hr: 0  min: 0  sec: 5\n",
      "epoch: 2  batch: 1085 / 1170  loss: 0.006830370255551743  hr: 0  min: 0  sec: 5\n",
      "epoch: 2  batch: 1086 / 1170  loss: 0.0068256889375749525  hr: 0  min: 0  sec: 5\n",
      "epoch: 2  batch: 1087 / 1170  loss: 0.0068232757683829234  hr: 0  min: 0  sec: 5\n",
      "epoch: 2  batch: 1088 / 1170  loss: 0.006817597806328944  hr: 0  min: 0  sec: 5\n",
      "epoch: 2  batch: 1089 / 1170  loss: 0.006811443461127997  hr: 0  min: 0  sec: 5\n",
      "epoch: 2  batch: 1090 / 1170  loss: 0.006820734299480629  hr: 0  min: 0  sec: 5\n",
      "epoch: 2  batch: 1091 / 1170  loss: 0.006814544916343529  hr: 0  min: 0  sec: 5\n",
      "epoch: 2  batch: 1092 / 1170  loss: 0.006808545339344907  hr: 0  min: 0  sec: 5\n",
      "epoch: 2  batch: 1093 / 1170  loss: 0.0068167637081589716  hr: 0  min: 0  sec: 5\n",
      "epoch: 2  batch: 1094 / 1170  loss: 0.006810646566215556  hr: 0  min: 0  sec: 4\n",
      "epoch: 2  batch: 1095 / 1170  loss: 0.006806134091720483  hr: 0  min: 0  sec: 4\n",
      "epoch: 2  batch: 1096 / 1170  loss: 0.006800685295135477  hr: 0  min: 0  sec: 4\n",
      "epoch: 2  batch: 1097 / 1170  loss: 0.006795855231936821  hr: 0  min: 0  sec: 4\n",
      "epoch: 2  batch: 1098 / 1170  loss: 0.006789779259544629  hr: 0  min: 0  sec: 4\n",
      "epoch: 2  batch: 1099 / 1170  loss: 0.006783895689586908  hr: 0  min: 0  sec: 4\n",
      "epoch: 2  batch: 1100 / 1170  loss: 0.006828788170241751  hr: 0  min: 0  sec: 4\n",
      "epoch: 2  batch: 1101 / 1170  loss: 0.0068230976888378715  hr: 0  min: 0  sec: 4\n",
      "epoch: 2  batch: 1102 / 1170  loss: 0.006817075840289727  hr: 0  min: 0  sec: 4\n",
      "epoch: 2  batch: 1103 / 1170  loss: 0.0068117831221108085  hr: 0  min: 0  sec: 4\n",
      "epoch: 2  batch: 1104 / 1170  loss: 0.006805674618550731  hr: 0  min: 0  sec: 4\n",
      "epoch: 2  batch: 1105 / 1170  loss: 0.006799660252269708  hr: 0  min: 0  sec: 4\n",
      "epoch: 2  batch: 1106 / 1170  loss: 0.006793591405410184  hr: 0  min: 0  sec: 4\n",
      "epoch: 2  batch: 1107 / 1170  loss: 0.006796153456795782  hr: 0  min: 0  sec: 4\n",
      "epoch: 2  batch: 1108 / 1170  loss: 0.006791446169565448  hr: 0  min: 0  sec: 4\n",
      "epoch: 2  batch: 1109 / 1170  loss: 0.0067855522981022905  hr: 0  min: 0  sec: 3\n",
      "epoch: 2  batch: 1110 / 1170  loss: 0.006779960564115853  hr: 0  min: 0  sec: 3\n",
      "epoch: 2  batch: 1111 / 1170  loss: 0.006805311604504076  hr: 0  min: 0  sec: 3\n",
      "epoch: 2  batch: 1112 / 1170  loss: 0.006799276427902847  hr: 0  min: 0  sec: 3\n",
      "epoch: 2  batch: 1113 / 1170  loss: 0.006793340813359511  hr: 0  min: 0  sec: 3\n",
      "epoch: 2  batch: 1114 / 1170  loss: 0.006787532528736066  hr: 0  min: 0  sec: 3\n",
      "epoch: 2  batch: 1115 / 1170  loss: 0.006781622150362388  hr: 0  min: 0  sec: 3\n",
      "epoch: 2  batch: 1116 / 1170  loss: 0.006775684244788094  hr: 0  min: 0  sec: 3\n",
      "epoch: 2  batch: 1117 / 1170  loss: 0.006770496740165213  hr: 0  min: 0  sec: 3\n",
      "epoch: 2  batch: 1118 / 1170  loss: 0.0067949659250333945  hr: 0  min: 0  sec: 3\n",
      "epoch: 2  batch: 1119 / 1170  loss: 0.006789290024717145  hr: 0  min: 0  sec: 3\n",
      "epoch: 2  batch: 1120 / 1170  loss: 0.0067834317025894314  hr: 0  min: 0  sec: 3\n",
      "epoch: 2  batch: 1121 / 1170  loss: 0.006791654453176644  hr: 0  min: 0  sec: 3\n",
      "epoch: 2  batch: 1122 / 1170  loss: 0.0068740973292539  hr: 0  min: 0  sec: 3\n",
      "epoch: 2  batch: 1123 / 1170  loss: 0.00686828208413718  hr: 0  min: 0  sec: 3\n",
      "epoch: 2  batch: 1124 / 1170  loss: 0.006864028665802582  hr: 0  min: 0  sec: 3\n",
      "epoch: 2  batch: 1125 / 1170  loss: 0.006862562371126842  hr: 0  min: 0  sec: 2\n",
      "epoch: 2  batch: 1126 / 1170  loss: 0.006856591423219378  hr: 0  min: 0  sec: 2\n",
      "epoch: 2  batch: 1127 / 1170  loss: 0.006850913971945399  hr: 0  min: 0  sec: 2\n",
      "epoch: 2  batch: 1128 / 1170  loss: 0.006845108843779289  hr: 0  min: 0  sec: 2\n",
      "epoch: 2  batch: 1129 / 1170  loss: 0.006843245896585073  hr: 0  min: 0  sec: 2\n",
      "epoch: 2  batch: 1130 / 1170  loss: 0.006838024566466436  hr: 0  min: 0  sec: 2\n",
      "epoch: 2  batch: 1131 / 1170  loss: 0.006832179795104505  hr: 0  min: 0  sec: 2\n",
      "epoch: 2  batch: 1132 / 1170  loss: 0.006826391768939339  hr: 0  min: 0  sec: 2\n",
      "epoch: 2  batch: 1133 / 1170  loss: 0.006862753529918418  hr: 0  min: 0  sec: 2\n",
      "epoch: 2  batch: 1134 / 1170  loss: 0.006858045998796222  hr: 0  min: 0  sec: 2\n",
      "epoch: 2  batch: 1135 / 1170  loss: 0.006852352043678153  hr: 0  min: 0  sec: 2\n",
      "epoch: 2  batch: 1136 / 1170  loss: 0.006854173480277636  hr: 0  min: 0  sec: 2\n",
      "epoch: 2  batch: 1137 / 1170  loss: 0.006848383238946348  hr: 0  min: 0  sec: 2\n",
      "epoch: 2  batch: 1138 / 1170  loss: 0.00684349775395121  hr: 0  min: 0  sec: 2\n",
      "epoch: 2  batch: 1139 / 1170  loss: 0.006837745563630119  hr: 0  min: 0  sec: 2\n",
      "epoch: 2  batch: 1140 / 1170  loss: 0.006831859616087106  hr: 0  min: 0  sec: 1\n",
      "epoch: 2  batch: 1141 / 1170  loss: 0.006837015312235065  hr: 0  min: 0  sec: 1\n",
      "epoch: 2  batch: 1142 / 1170  loss: 0.006832069900234105  hr: 0  min: 0  sec: 1\n",
      "epoch: 2  batch: 1143 / 1170  loss: 0.006826626786050639  hr: 0  min: 0  sec: 1\n",
      "epoch: 2  batch: 1144 / 1170  loss: 0.00682101311899301  hr: 0  min: 0  sec: 1\n",
      "epoch: 2  batch: 1145 / 1170  loss: 0.006815745475287417  hr: 0  min: 0  sec: 1\n",
      "epoch: 2  batch: 1146 / 1170  loss: 0.006814717965230866  hr: 0  min: 0  sec: 1\n",
      "epoch: 2  batch: 1147 / 1170  loss: 0.006810949424297398  hr: 0  min: 0  sec: 1\n",
      "epoch: 2  batch: 1148 / 1170  loss: 0.006805805759170048  hr: 0  min: 0  sec: 1\n",
      "epoch: 2  batch: 1149 / 1170  loss: 0.006813806469912762  hr: 0  min: 0  sec: 1\n",
      "epoch: 2  batch: 1150 / 1170  loss: 0.006808385733146798  hr: 0  min: 0  sec: 1\n",
      "epoch: 2  batch: 1151 / 1170  loss: 0.006804022081997612  hr: 0  min: 0  sec: 1\n",
      "epoch: 2  batch: 1152 / 1170  loss: 0.0067983269907409116  hr: 0  min: 0  sec: 1\n",
      "epoch: 2  batch: 1153 / 1170  loss: 0.0068009207418627355  hr: 0  min: 0  sec: 1\n",
      "epoch: 2  batch: 1154 / 1170  loss: 0.006842898235356859  hr: 0  min: 0  sec: 1\n",
      "epoch: 2  batch: 1155 / 1170  loss: 0.006841268867537232  hr: 0  min: 0  sec: 0\n",
      "epoch: 2  batch: 1156 / 1170  loss: 0.006836613249219742  hr: 0  min: 0  sec: 0\n",
      "epoch: 2  batch: 1157 / 1170  loss: 0.006835862412387833  hr: 0  min: 0  sec: 0\n",
      "epoch: 2  batch: 1158 / 1170  loss: 0.006830567779009773  hr: 0  min: 0  sec: 0\n",
      "epoch: 2  batch: 1159 / 1170  loss: 0.006825720071596484  hr: 0  min: 0  sec: 0\n",
      "epoch: 2  batch: 1160 / 1170  loss: 0.006820593694853948  hr: 0  min: 0  sec: 0\n",
      "epoch: 2  batch: 1161 / 1170  loss: 0.006817254626502142  hr: 0  min: 0  sec: 0\n",
      "epoch: 2  batch: 1162 / 1170  loss: 0.006817733070298117  hr: 0  min: 0  sec: 0\n",
      "epoch: 2  batch: 1163 / 1170  loss: 0.006812160666872596  hr: 0  min: 0  sec: 0\n",
      "epoch: 2  batch: 1164 / 1170  loss: 0.006814404929591984  hr: 0  min: 0  sec: 0\n",
      "epoch: 2  batch: 1165 / 1170  loss: 0.006808847554152693  hr: 0  min: 0  sec: 0\n",
      "epoch: 2  batch: 1166 / 1170  loss: 0.006803173722080416  hr: 0  min: 0  sec: 0\n",
      "epoch: 2  batch: 1167 / 1170  loss: 0.006799279546239998  hr: 0  min: 0  sec: 0\n",
      "epoch: 2  batch: 1168 / 1170  loss: 0.00679391922352034  hr: 0  min: 0  sec: 0\n",
      "epoch: 2  batch: 1169 / 1170  loss: 0.006788394985850922  hr: 0  min: 0  sec: 0\n",
      "epoch: 2  batch: 1170 / 1170  loss: 0.006805209184216452  hr: 0  min: 0  sec: 0\n"
     ]
    }
   ],
   "source": [
    "train(train_loader, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# import seaborn as sn\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model('bert_aspect_extraction_2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_c_matrix(c_matrix, title=\"test\"):\n",
    "    aix = []\n",
    "    for y in range(len(c_matrix)):\n",
    "        aix.append(y)\n",
    "    df_cm = pd.DataFrame(c_matrix, aix, aix)\n",
    "    sn.heatmap(df_cm, annot=True, fmt='g')\n",
    "    plt.ylabel(\"prediction\")\n",
    "    plt.xlabel(\"ground truth\")\n",
    "    plt.title(title)\n",
    "    plt.savefig(title+'.jpg')\n",
    "    plt.show()\n",
    "\n",
    "def test(loader):\n",
    "    pred = []\n",
    "    trueth = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "\n",
    "            ids_tensors, tags_tensors, _, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(DEVICE)\n",
    "            tags_tensors = tags_tensors.to(DEVICE)\n",
    "            masks_tensors = masks_tensors.to(DEVICE)\n",
    "\n",
    "            outputs = model(ids_tensors=ids_tensors, tags_tensors=None, masks_tensors=masks_tensors)\n",
    "            \n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "            pred += list([int(j) for i in predictions for j in i ])\n",
    "            trueth += list([int(j) for i in tags_tensors for j in i ])\n",
    "\n",
    "    return trueth, pred\n",
    "\n",
    "def predict(sentence, tokenizer):\n",
    "    word_pieces = []\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    word_pieces += tokens\n",
    "\n",
    "    ids = tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "    input_tensor = torch.tensor([ids]).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor, None, None)\n",
    "        _, predictions = torch.max(outputs, dim=2)\n",
    "    predictions = predictions[0].tolist()\n",
    "    \n",
    "    return word_pieces, predictions, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 23.7 s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99    130191\n",
      "           1       0.95      0.88      0.91      8871\n",
      "\n",
      "    accuracy                           0.99    139062\n",
      "   macro avg       0.97      0.94      0.95    139062\n",
      "weighted avg       0.99      0.99      0.99    139062\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%time x, y = test(test_loader)\n",
    "print(classification_report(x, y, target_names=[str(i) for i in range(2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('not', 0) ('impressed', 0) ('with', 0) ('this', 0) ('movie', 0) (',', 0) ('but', 0) ('the', 0) ('actors', 1) ('performance', 1) ('well', 0) ('.', 0) "
     ]
    }
   ],
   "source": [
    "x, y, z = predict(\"Not impressed with this movie, but the actors performance well.\", tokenizer)\n",
    "for i in range(len(x)):\n",
    "    print((x[i], y[i]), end=\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 0) ('movie', 0) ('is', 0) ('terrible', 0) (',', 0) ('but', 0) ('the', 0) ('actors', 1) ('performance', 0) ('well', 0) ('.', 0) "
     ]
    }
   ],
   "source": [
    "x, y, z = predict(\"The movie is terrible, but the actors performance well.\", tokenizer)\n",
    "for i in range(len(x)):\n",
    "    print((x[i], y[i]), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('movie', 1) ('is', 0) ('terrible', 0) (',', 0) ('but', 0) ('the', 0) ('actors', 1) ('performance', 1) ('well', 0) ('.', 0) "
     ]
    }
   ],
   "source": [
    "x, y, z = predict(\"Movie is terrible, but the actors performance well.\", tokenizer)\n",
    "for i in range(len(x)):\n",
    "    print((x[i], y[i]), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 0) ('am', 1) ('##d', 1) ('ry', 1) ('##zen', 1) ('is', 0) ('really', 0) ('fast', 0) ('!', 0) "
     ]
    }
   ],
   "source": [
    "x, y, z = predict(\"The AMD Ryzen is really fast !\", tokenizer)\n",
    "for i in range(len(x)):\n",
    "    print((x[i], y[i]), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('for', 0) ('the', 0) ('price', 1) ('you', 0) ('pay', 0) ('this', 0) ('product', 0) ('is', 0) ('very', 0) ('good', 0) ('.', 0) ('however', 0) (',', 0) ('battery', 1) ('life', 1) ('is', 0) ('a', 0) ('little', 0) ('lack', 0) ('-', 0) ('lust', 0) ('##er', 0) ('coming', 0) ('from', 0) ('a', 0) ('mac', 0) ('##book', 0) ('pro', 0) ('.', 0) "
     ]
    }
   ],
   "source": [
    "x, y, z = predict(\"For the price you pay this product is very good. However, battery life is a little lack-luster coming from a MacBook Pro.\", tokenizer)\n",
    "for i in range(len(x)):\n",
    "    print((x[i], y[i]), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('for', 0) ('the', 0) ('elephants', 1) ('you', 0) ('pay', 0) ('this', 0) ('product', 0) ('is', 0) ('very', 0) ('good', 0) ('.', 0) ('however', 0) (',', 0) ('lions', 1) ('is', 0) ('a', 0) ('little', 0) ('lack', 0) ('-', 0) ('lust', 0) ('##er', 0) ('coming', 0) ('from', 0) ('a', 0) ('mac', 0) ('##book', 0) ('pro', 0) ('.', 0) "
     ]
    }
   ],
   "source": [
    "x, y, z = predict(\"For the elephants you pay this product is very good. However, lions is a little lack-luster coming from a MacBook Pro.\", tokenizer)\n",
    "for i in range(len(x)):\n",
    "    print((x[i], y[i]), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('for', 0) ('the', 0) ('java', 1) ('you', 0) ('pay', 0) ('this', 0) ('product', 0) ('is', 0) ('very', 0) ('good', 0) ('.', 0) ('however', 0) (',', 0) ('python', 1) ('is', 0) ('a', 0) ('little', 0) ('lack', 0) ('-', 0) ('lust', 0) ('##er', 0) ('coming', 0) ('from', 0) ('a', 0) ('mac', 0) ('##book', 0) ('pro', 0) ('.', 0) "
     ]
    }
   ],
   "source": [
    "x, y, z = predict(\"For the java you pay this product is very good. However, python is a little lack-luster coming from a MacBook Pro.\", tokenizer)\n",
    "for i in range(len(x)):\n",
    "    print((x[i], y[i]), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('apple', 0) ('is', 0) ('better', 0) ('than', 0) ('microsoft', 0) ('.', 0) "
     ]
    }
   ],
   "source": [
    "x, y, z = predict(\"Apple is better than Microsoft.\", tokenizer)\n",
    "for i in range(len(x)):\n",
    "    print((x[i], y[i]), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('apple', 0) ('is', 0) ('better', 0) ('than', 0) ('microsoft', 1) ('.', 0) ('microsoft', 1) ('is', 0) ('bad', 0) ('.', 0) "
     ]
    }
   ],
   "source": [
    "x, y, z = predict(\"Apple is better than Microsoft. Microsoft is bad.\", tokenizer)\n",
    "for i in range(len(x)):\n",
    "    print((x[i], y[i]), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('apple', 0) ('is', 0) ('better', 0) ('than', 0) ('microsoft', 1) ('.', 0) ('apple', 1) ('is', 0) ('good', 0) ('.', 0) "
     ]
    }
   ],
   "source": [
    "x, y, z = predict(\"Apple is better than Microsoft. Apple is good.\", tokenizer)\n",
    "for i in range(len(x)):\n",
    "    print((x[i], y[i]), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('am', 0) ('##d', 0) (\"'\", 0) ('s', 0) ('cpu', 1) ('is', 0) ('better', 0) ('than', 0) ('intel', 0) (\"'\", 0) ('s', 0) ('cpu', 1) "
     ]
    }
   ],
   "source": [
    "x, y, z = predict(\"AMD's cpu is better than Intel's cpu\", tokenizer)\n",
    "for i in range(len(x)):\n",
    "    print((x[i], y[i]), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('am', 1) ('##d', 1) ('is', 0) ('better', 0) ('than', 0) ('intel', 0) ('.', 0) "
     ]
    }
   ],
   "source": [
    "x, y, z = predict(\"AMD is better than Intel.\", tokenizer)\n",
    "for i in range(len(x)):\n",
    "    print((x[i], y[i]), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('am', 1) ('##d', 1) ('is', 0) ('better', 0) ('than', 0) ('intel', 1) ('.', 0) ('intel', 1) ('is', 0) ('so', 0) ('expensive', 0) "
     ]
    }
   ],
   "source": [
    "x, y, z = predict(\"AMD is better than Intel. Intel is so expensive\", tokenizer)\n",
    "for i in range(len(x)):\n",
    "    print((x[i], y[i]), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('am', 1) ('##d', 1) ('is', 0) ('better', 0) ('than', 0) ('intel', 1) ('.', 0) ('it', 0) ('is', 0) ('so', 0) ('expensive', 0) "
     ]
    }
   ],
   "source": [
    "x, y, z = predict(\"AMD is better than Intel. It is so expensive\", tokenizer)\n",
    "for i in range(len(x)):\n",
    "    print((x[i], y[i]), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i', 0) ('think', 0) ('apple', 0) ('is', 0) ('better', 0) ('than', 0) ('microsoft', 0) "
     ]
    }
   ],
   "source": [
    "x, y, z = predict(\"I think Apple is better than Microsoft\", tokenizer)\n",
    "for i in range(len(x)):\n",
    "    print((x[i], y[i]), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cpu', 1) ('and', 0) ('graphic', 1) ('card', 1) ('are', 0) ('strong', 0) "
     ]
    }
   ],
   "source": [
    "x, y, z = predict(\"cpu and graphic card are strong\", tokenizer)\n",
    "for i in range(len(x)):\n",
    "    print((x[i], y[i]), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('dogs', 1) ('and', 0) ('cats', 1) ('are', 0) ('cute', 0) "
     ]
    }
   ],
   "source": [
    "x, y, z = predict(\"dogs and cats are cute\", tokenizer)\n",
    "for i in range(len(x)):\n",
    "    print((x[i], y[i]), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('python', 0) (\"'\", 0) ('s', 0) ('design', 0) ('philosophy', 0) ('emphasizes', 0) ('code', 1) ('read', 1) ('##ability', 1) ('with', 0) ('its', 0) ('notable', 0) ('use', 0) ('of', 0) ('significant', 0) ('whites', 1) ('##pace', 0) "
     ]
    }
   ],
   "source": [
    "x, y, z = predict(\"Python's design philosophy emphasizes code readability with its notable use of significant whitespace\", tokenizer)\n",
    "for i in range(len(x)):\n",
    "    print((x[i], y[i]), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i', 0) ('have', 0) ('to', 0) ('say', 0) ('they', 0) ('have', 0) ('one', 0) ('of', 0) ('the', 0) ('fastest', 0) ('delivery', 1) ('times', 1) ('in', 0) ('the', 0) ('city', 0) "
     ]
    }
   ],
   "source": [
    "x, y, z = predict(\"I have to say they have one of the fastest delivery times in the city\", tokenizer)\n",
    "for i in range(len(x)):\n",
    "    print((x[i], y[i]), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i', 0) ('have', 0) ('to', 0) ('say', 0) ('they', 0) ('have', 0) ('one', 0) ('of', 0) ('the', 0) ('fastest', 0) ('as', 1) ('##ib', 1) ('-', 1) ('ki', 1) ('##u', 1) ('in', 0) ('the', 0) ('city', 0) "
     ]
    }
   ],
   "source": [
    "x, y, z = predict(\"I have to say they have one of the fastest ASIB-KIU in the city\", tokenizer)\n",
    "for i in range(len(x)):\n",
    "    print((x[i], y[i]), end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* need NLI preprocessing (it, this, he...) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
