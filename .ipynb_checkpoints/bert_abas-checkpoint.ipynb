{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class absa_dataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens, tags, pols = self.df.iloc[idx, :3].values\n",
    "        tokens = tokens.replace(\"'\", \"\").strip(\"][\").split(', ')\n",
    "        tags = tags.strip('][').split(', ')\n",
    "        pols = pols.strip('][').split(', ')\n",
    "\n",
    "        bert_tokens = []\n",
    "        bert_att = []\n",
    "        pols_label = 0\n",
    "        for i in range(len(tokens)):\n",
    "            t = tokenizer.tokenize(tokens[i])\n",
    "            bert_tokens += t\n",
    "            if int(pols[i]) != -1:\n",
    "                bert_att += t\n",
    "                pols_label = int(pols[i])\n",
    "\n",
    "        segment_tensor = [0] + [0]*len(bert_tokens) + [0] + [1]*len(bert_att)\n",
    "        bert_tokens = ['[cls]'] + bert_tokens + ['[sep]'] + bert_att\n",
    "        \n",
    "\n",
    "        bert_ids = tokenizer.convert_tokens_to_ids(bert_tokens)\n",
    "\n",
    "        ids_tensor = torch.tensor(bert_ids)\n",
    "        pols_tensor = torch.tensor(pols_label)\n",
    "        segment_tensor = torch.tensor(segment_tensor)\n",
    "\n",
    "        return bert_tokens, ids_tensor, segment_tensor, pols_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"laptops_train.csv\")\n",
    "laptops_train_ds = absa_dataset(df, tokenizer)\n",
    "\n",
    "df = pd.read_csv(\"laptops_test.csv\")\n",
    "laptops_test_ds = absa_dataset(df, tokenizer)\n",
    "\n",
    "df = pd.read_csv(\"restaurants_train.csv\")\n",
    "restaurants_train_ds = absa_dataset(df, tokenizer)\n",
    "\n",
    "df = pd.read_csv(\"restaurants_test.csv\")\n",
    "restaurants_test_ds = absa_dataset(df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[cls]', 'the', 'battery', 'life', 'seems', 'to', 'be', 'very', 'good', ',', 'and', 'have', 'had', 'no', 'issues', 'with', 'it', '.', '[sep]', 'battery', 'life']\n",
      "21\n",
      "tensor([ 100, 1996, 6046, 2166, 3849, 2000, 2022, 2200, 2204, 1010, 1998, 2031,\n",
      "        2018, 2053, 3314, 2007, 2009, 1012,  100, 6046, 2166])\n",
      "21\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1])\n",
      "21\n",
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "w,x,y,z = laptops_train_ds.__getitem__(121)\n",
    "print(w)\n",
    "print(len(w))\n",
    "print(x)\n",
    "print(len(x))\n",
    "print(y)\n",
    "print(len(y))\n",
    "print(z)\n",
    "# print(len(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True)\n",
    "\n",
    "    segments_tensors = [s[2] for s in samples]\n",
    "    segments_tensors = pad_sequence(segments_tensors, batch_first=True)\n",
    "\n",
    "    label_ids = torch.stack([s[3] for s in samples])\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1)\n",
    "\n",
    "    \n",
    "    return ids_tensors, segments_tensors, masks_tensors, label_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ConcatDataset([laptops_train_ds, restaurants_train_ds])\n",
    "test_ds = ConcatDataset([laptops_test_ds, restaurants_test_ds])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=4, collate_fn=create_mini_batch, shuffle = True)\n",
    "test_loader = DataLoader(test_ds, batch_size=50, collate_fn=create_mini_batch, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  100,  1996,  2048,  2732, 27828,  2187,  3243,  2070,  2051,  3283,\n",
      "          2000,  2330,  2037,  2219,  2173,  1012,   100, 27828,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [  100,  1996,  2833,  2467, 16958,  4840,  1998,  2366, 13364,  1012,\n",
      "           100,  2833,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [  100,  3811, 16755,  2023,  2004,  2307,  3643,  2005,  6581, 10514,\n",
      "          6182,  1998,  2326,  1012,   100,  2326,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [  100,  2174,  1010,  1045,  2001,  2045,  2005,  1037,  2147,  4596,\n",
      "          2025,  2146,  3283,  2043,  2026, 11729,  2013,  2414,  4384,  1037,\n",
      "          2200,  2312,  2300,  8569,  2290,  2006,  1996,  5894,  1012,   100,\n",
      "          4596]])\n",
      "torch.Size([4, 31])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 1]])\n",
      "torch.Size([4, 31])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1]])\n",
      "torch.Size([4, 31])\n",
      "tensor([1, 2, 2, 1])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    w,x,y,z = batch\n",
    "    print(w)\n",
    "    print(w.size())\n",
    "    print(x)\n",
    "    print(x.size())\n",
    "    print(y)\n",
    "    print(y.size())\n",
    "    print(z)\n",
    "    print(z.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bert aspect extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bert_ABSA(torch.nn.Module):\n",
    "    def __init__(self, pretrain_model):\n",
    "        super(bert_ABSA, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(pretrain_model)\n",
    "        self.linear = torch.nn.Linear(self.bert.config.hidden_size, 3)\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, ids_tensors, lable_tensors, masks_tensors, segments_tensors):\n",
    "        _, pooled_outputs = self.bert(input_ids=ids_tensors, attention_mask=masks_tensors, token_type_ids=segments_tensors)\n",
    "        # print(bert_outputs.size())\n",
    "        linear_outputs = self.linear(pooled_outputs)\n",
    "        # print(linear_outputs.size())\n",
    "\n",
    "        if lable_tensors is not None:\n",
    "            # print(linear_outputs.size())\n",
    "            # print(tags_tensors.size())\n",
    "            loss = self.loss_fn(linear_outputs, lable_tensors)\n",
    "            return loss\n",
    "        else:\n",
    "            return linear_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "lr = 2e-5\n",
    "model = bert_ABSA(\"bert-base-uncased\").to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evl_time(t):\n",
    "    min, sec= divmod(t, 60)\n",
    "    hr, min = divmod(min, 60)\n",
    "    return int(hr), int(min), int(sec)\n",
    "\n",
    "def load_model(path):\n",
    "    model.load_state_dict(torch.load(path), strict=False)\n",
    "    \n",
    "def save_model(name):\n",
    "    torch.save(model.state_dict(), name)\n",
    "\n",
    "def train(loader, epochs):\n",
    "    all_data = len(loader)\n",
    "    for epoch in range(epochs):\n",
    "        finish_data = 0\n",
    "        losses = []\n",
    "        current_times = []\n",
    "        correct_predictions = 0\n",
    "        \n",
    "        for data in loader:\n",
    "            t0 = time.time()\n",
    "            ids_tensors, segments_tensors, masks_tensors, label_ids = data\n",
    "            ids_tensors = ids_tensors.to(DEVICE)\n",
    "            segments_tensors = segments_tensors.to(DEVICE)\n",
    "            label_ids = label_ids.to(DEVICE)\n",
    "            masks_tensors = masks_tensors.to(DEVICE)\n",
    "\n",
    "            loss = model(ids_tensors=ids_tensors, lable_tensors=label_ids, masks_tensors=masks_tensors, segments_tensors=segments_tensors)\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            finish_data += 1\n",
    "            current_times.append(round(time.time()-t0,3))\n",
    "            current = np.mean(current_times)\n",
    "            hr, min, sec = evl_time(current*(all_data-finish_data) + current*all_data*(epochs-epoch-1))\n",
    "            print('epoch:', epoch, \" batch:\", finish_data, \"/\" , all_data, \" loss:\", np.mean(losses), \" hr:\", hr, \" min:\", min,\" sec:\", sec)         \n",
    "\n",
    "        save_model('train-epoch-temp.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149355006318  hr: 0  min: 0  sec: 16\n",
      "epoch: 5  batch: 1212 / 1463  loss: 0.06295723273701141  hr: 0  min: 0  sec: 16\n",
      "epoch: 5  batch: 1213 / 1463  loss: 0.06291003511636134  hr: 0  min: 0  sec: 16\n",
      "epoch: 5  batch: 1214 / 1463  loss: 0.06286994091632006  hr: 0  min: 0  sec: 16\n",
      "epoch: 5  batch: 1215 / 1463  loss: 0.0631474026678841  hr: 0  min: 0  sec: 16\n",
      "epoch: 5  batch: 1216 / 1463  loss: 0.06309828165570396  hr: 0  min: 0  sec: 16\n",
      "epoch: 5  batch: 1217 / 1463  loss: 0.06305265262035287  hr: 0  min: 0  sec: 16\n",
      "epoch: 5  batch: 1218 / 1463  loss: 0.0630028611862195  hr: 0  min: 0  sec: 15\n",
      "epoch: 5  batch: 1219 / 1463  loss: 0.06297745786395159  hr: 0  min: 0  sec: 15\n",
      "epoch: 5  batch: 1220 / 1463  loss: 0.06307165179672158  hr: 0  min: 0  sec: 15\n",
      "epoch: 5  batch: 1221 / 1463  loss: 0.06302445060405236  hr: 0  min: 0  sec: 15\n",
      "epoch: 5  batch: 1222 / 1463  loss: 0.06297631300524248  hr: 0  min: 0  sec: 15\n",
      "epoch: 5  batch: 1223 / 1463  loss: 0.06292759960403346  hr: 0  min: 0  sec: 15\n",
      "epoch: 5  batch: 1224 / 1463  loss: 0.06287853054791591  hr: 0  min: 0  sec: 15\n",
      "epoch: 5  batch: 1225 / 1463  loss: 0.06284121755517696  hr: 0  min: 0  sec: 15\n",
      "epoch: 5  batch: 1226 / 1463  loss: 0.06279354318368213  hr: 0  min: 0  sec: 15\n",
      "epoch: 5  batch: 1227 / 1463  loss: 0.06274458608531147  hr: 0  min: 0  sec: 15\n",
      "epoch: 5  batch: 1228 / 1463  loss: 0.06269519770311185  hr: 0  min: 0  sec: 15\n",
      "epoch: 5  batch: 1229 / 1463  loss: 0.06264999797165624  hr: 0  min: 0  sec: 15\n",
      "epoch: 5  batch: 1230 / 1463  loss: 0.06260534067213729  hr: 0  min: 0  sec: 15\n",
      "epoch: 5  batch: 1231 / 1463  loss: 0.0625560044133527  hr: 0  min: 0  sec: 15\n",
      "epoch: 5  batch: 1232 / 1463  loss: 0.06251643986320528  hr: 0  min: 0  sec: 15\n",
      "epoch: 5  batch: 1233 / 1463  loss: 0.0631702639590388  hr: 0  min: 0  sec: 14\n",
      "epoch: 5  batch: 1234 / 1463  loss: 0.06356464102973228  hr: 0  min: 0  sec: 14\n",
      "epoch: 5  batch: 1235 / 1463  loss: 0.06352224757860467  hr: 0  min: 0  sec: 14\n",
      "epoch: 5  batch: 1236 / 1463  loss: 0.06378316932382243  hr: 0  min: 0  sec: 14\n",
      "epoch: 5  batch: 1237 / 1463  loss: 0.06373464244932352  hr: 0  min: 0  sec: 14\n",
      "epoch: 5  batch: 1238 / 1463  loss: 0.06378000665794864  hr: 0  min: 0  sec: 14\n",
      "epoch: 5  batch: 1239 / 1463  loss: 0.06373167739150075  hr: 0  min: 0  sec: 14\n",
      "epoch: 5  batch: 1240 / 1463  loss: 0.06371826370602995  hr: 0  min: 0  sec: 14\n",
      "epoch: 5  batch: 1241 / 1463  loss: 0.06366858500405215  hr: 0  min: 0  sec: 14\n",
      "epoch: 5  batch: 1242 / 1463  loss: 0.06361854233412614  hr: 0  min: 0  sec: 14\n",
      "epoch: 5  batch: 1243 / 1463  loss: 0.06356859194750827  hr: 0  min: 0  sec: 14\n",
      "epoch: 5  batch: 1244 / 1463  loss: 0.06351885547826551  hr: 0  min: 0  sec: 14\n",
      "epoch: 5  batch: 1245 / 1463  loss: 0.06347579337124915  hr: 0  min: 0  sec: 14\n",
      "epoch: 5  batch: 1246 / 1463  loss: 0.0634279845060396  hr: 0  min: 0  sec: 14\n",
      "epoch: 5  batch: 1247 / 1463  loss: 0.06338701126080827  hr: 0  min: 0  sec: 14\n",
      "epoch: 5  batch: 1248 / 1463  loss: 0.06335436626350435  hr: 0  min: 0  sec: 13\n",
      "epoch: 5  batch: 1249 / 1463  loss: 0.06330513427047722  hr: 0  min: 0  sec: 13\n",
      "epoch: 5  batch: 1250 / 1463  loss: 0.06325745652324986  hr: 0  min: 0  sec: 13\n",
      "epoch: 5  batch: 1251 / 1463  loss: 0.06375628669896881  hr: 0  min: 0  sec: 13\n",
      "epoch: 5  batch: 1252 / 1463  loss: 0.06377239075433831  hr: 0  min: 0  sec: 13\n",
      "epoch: 5  batch: 1253 / 1463  loss: 0.06372744588750259  hr: 0  min: 0  sec: 13\n",
      "epoch: 5  batch: 1254 / 1463  loss: 0.06367761734081637  hr: 0  min: 0  sec: 13\n",
      "epoch: 5  batch: 1255 / 1463  loss: 0.06362964424489723  hr: 0  min: 0  sec: 13\n",
      "epoch: 5  batch: 1256 / 1463  loss: 0.06358920377304249  hr: 0  min: 0  sec: 13\n",
      "epoch: 5  batch: 1257 / 1463  loss: 0.06372827998267182  hr: 0  min: 0  sec: 13\n",
      "epoch: 5  batch: 1258 / 1463  loss: 0.06367994967906085  hr: 0  min: 0  sec: 13\n",
      "epoch: 5  batch: 1259 / 1463  loss: 0.06363203530865122  hr: 0  min: 0  sec: 13\n",
      "epoch: 5  batch: 1260 / 1463  loss: 0.06361863472864368  hr: 0  min: 0  sec: 13\n",
      "epoch: 5  batch: 1261 / 1463  loss: 0.06356910557275536  hr: 0  min: 0  sec: 13\n",
      "epoch: 5  batch: 1262 / 1463  loss: 0.0635351547253458  hr: 0  min: 0  sec: 13\n",
      "epoch: 5  batch: 1263 / 1463  loss: 0.06348600763893836  hr: 0  min: 0  sec: 12\n",
      "epoch: 5  batch: 1264 / 1463  loss: 0.06363475529929306  hr: 0  min: 0  sec: 12\n",
      "epoch: 5  batch: 1265 / 1463  loss: 0.06358591394474557  hr: 0  min: 0  sec: 12\n",
      "epoch: 5  batch: 1266 / 1463  loss: 0.06355821981061582  hr: 0  min: 0  sec: 12\n",
      "epoch: 5  batch: 1267 / 1463  loss: 0.06351127788192856  hr: 0  min: 0  sec: 12\n",
      "epoch: 5  batch: 1268 / 1463  loss: 0.0634628729266546  hr: 0  min: 0  sec: 12\n",
      "epoch: 5  batch: 1269 / 1463  loss: 0.0634164520094369  hr: 0  min: 0  sec: 12\n",
      "epoch: 5  batch: 1270 / 1463  loss: 0.06336730872196104  hr: 0  min: 0  sec: 12\n",
      "epoch: 5  batch: 1271 / 1463  loss: 0.06344673878016016  hr: 0  min: 0  sec: 12\n",
      "epoch: 5  batch: 1272 / 1463  loss: 0.0634066754055483  hr: 0  min: 0  sec: 12\n",
      "epoch: 5  batch: 1273 / 1463  loss: 0.06335828014424394  hr: 0  min: 0  sec: 12\n",
      "epoch: 5  batch: 1274 / 1463  loss: 0.0641953537608283  hr: 0  min: 0  sec: 12\n",
      "epoch: 5  batch: 1275 / 1463  loss: 0.06422315470223754  hr: 0  min: 0  sec: 12\n",
      "epoch: 5  batch: 1276 / 1463  loss: 0.06475878442749568  hr: 0  min: 0  sec: 12\n",
      "epoch: 5  batch: 1277 / 1463  loss: 0.06470886645364114  hr: 0  min: 0  sec: 12\n",
      "epoch: 5  batch: 1278 / 1463  loss: 0.06465887469672675  hr: 0  min: 0  sec: 11\n",
      "epoch: 5  batch: 1279 / 1463  loss: 0.06460957958519462  hr: 0  min: 0  sec: 11\n",
      "epoch: 5  batch: 1280 / 1463  loss: 0.06456679567979791  hr: 0  min: 0  sec: 11\n",
      "epoch: 5  batch: 1281 / 1463  loss: 0.06451907087239483  hr: 0  min: 0  sec: 11\n",
      "epoch: 5  batch: 1282 / 1463  loss: 0.06446929332728381  hr: 0  min: 0  sec: 11\n",
      "epoch: 5  batch: 1283 / 1463  loss: 0.06442092145564574  hr: 0  min: 0  sec: 11\n",
      "epoch: 5  batch: 1284 / 1463  loss: 0.0643716891339229  hr: 0  min: 0  sec: 11\n",
      "epoch: 5  batch: 1285 / 1463  loss: 0.06432224231500791  hr: 0  min: 0  sec: 11\n",
      "epoch: 5  batch: 1286 / 1463  loss: 0.06430005809701099  hr: 0  min: 0  sec: 11\n",
      "epoch: 5  batch: 1287 / 1463  loss: 0.06473023959384597  hr: 0  min: 0  sec: 11\n",
      "epoch: 5  batch: 1288 / 1463  loss: 0.0646813469428487  hr: 0  min: 0  sec: 11\n",
      "epoch: 5  batch: 1289 / 1463  loss: 0.06463746004387945  hr: 0  min: 0  sec: 11\n",
      "epoch: 5  batch: 1290 / 1463  loss: 0.06462659572258812  hr: 0  min: 0  sec: 11\n",
      "epoch: 5  batch: 1291 / 1463  loss: 0.06460206134246693  hr: 0  min: 0  sec: 11\n",
      "epoch: 5  batch: 1292 / 1463  loss: 0.06458045482944468  hr: 0  min: 0  sec: 11\n",
      "epoch: 5  batch: 1293 / 1463  loss: 0.06454308721506  hr: 0  min: 0  sec: 11\n",
      "epoch: 5  batch: 1294 / 1463  loss: 0.0645996277335748  hr: 0  min: 0  sec: 10\n",
      "epoch: 5  batch: 1295 / 1463  loss: 0.06490427704204799  hr: 0  min: 0  sec: 10\n",
      "epoch: 5  batch: 1296 / 1463  loss: 0.06498986489332535  hr: 0  min: 0  sec: 10\n",
      "epoch: 5  batch: 1297 / 1463  loss: 0.06506971548266624  hr: 0  min: 0  sec: 10\n",
      "epoch: 5  batch: 1298 / 1463  loss: 0.06502151102300188  hr: 0  min: 0  sec: 10\n",
      "epoch: 5  batch: 1299 / 1463  loss: 0.06519664803282264  hr: 0  min: 0  sec: 10\n",
      "epoch: 5  batch: 1300 / 1463  loss: 0.06514723938520407  hr: 0  min: 0  sec: 10\n",
      "epoch: 5  batch: 1301 / 1463  loss: 0.0650982571605206  hr: 0  min: 0  sec: 10\n",
      "epoch: 5  batch: 1302 / 1463  loss: 0.06546378106423086  hr: 0  min: 0  sec: 10\n",
      "epoch: 5  batch: 1303 / 1463  loss: 0.06541563871300798  hr: 0  min: 0  sec: 10\n",
      "epoch: 5  batch: 1304 / 1463  loss: 0.06540356425064858  hr: 0  min: 0  sec: 10\n",
      "epoch: 5  batch: 1305 / 1463  loss: 0.06535405206832574  hr: 0  min: 0  sec: 10\n",
      "epoch: 5  batch: 1306 / 1463  loss: 0.0653058882884688  hr: 0  min: 0  sec: 10\n",
      "epoch: 5  batch: 1307 / 1463  loss: 0.06526114878642576  hr: 0  min: 0  sec: 10\n",
      "epoch: 5  batch: 1308 / 1463  loss: 0.0652130937086237  hr: 0  min: 0  sec: 10\n",
      "epoch: 5  batch: 1309 / 1463  loss: 0.06541922882937355  hr: 0  min: 0  sec: 9\n",
      "epoch: 5  batch: 1310 / 1463  loss: 0.06539493563826257  hr: 0  min: 0  sec: 9\n",
      "epoch: 5  batch: 1311 / 1463  loss: 0.0653614173334937  hr: 0  min: 0  sec: 9\n",
      "epoch: 5  batch: 1312 / 1463  loss: 0.06531314147701867  hr: 0  min: 0  sec: 9\n",
      "epoch: 5  batch: 1313 / 1463  loss: 0.06526443579051802  hr: 0  min: 0  sec: 9\n",
      "epoch: 5  batch: 1314 / 1463  loss: 0.06528376956355401  hr: 0  min: 0  sec: 9\n",
      "epoch: 5  batch: 1315 / 1463  loss: 0.06523914616818348  hr: 0  min: 0  sec: 9\n",
      "epoch: 5  batch: 1316 / 1463  loss: 0.06519676866287158  hr: 0  min: 0  sec: 9\n",
      "epoch: 5  batch: 1317 / 1463  loss: 0.06515671715937925  hr: 0  min: 0  sec: 9\n",
      "epoch: 5  batch: 1318 / 1463  loss: 0.0651740951720215  hr: 0  min: 0  sec: 9\n",
      "epoch: 5  batch: 1319 / 1463  loss: 0.06518448138881988  hr: 0  min: 0  sec: 9\n",
      "epoch: 5  batch: 1320 / 1463  loss: 0.06513965583887484  hr: 0  min: 0  sec: 9\n",
      "epoch: 5  batch: 1321 / 1463  loss: 0.06509187068142203  hr: 0  min: 0  sec: 9\n",
      "epoch: 5  batch: 1322 / 1463  loss: 0.06523736596313807  hr: 0  min: 0  sec: 9\n",
      "epoch: 5  batch: 1323 / 1463  loss: 0.06519279442443787  hr: 0  min: 0  sec: 9\n",
      "epoch: 5  batch: 1324 / 1463  loss: 0.06514988800021802  hr: 0  min: 0  sec: 8\n",
      "epoch: 5  batch: 1325 / 1463  loss: 0.06510136335251308  hr: 0  min: 0  sec: 8\n",
      "epoch: 5  batch: 1326 / 1463  loss: 0.06505316281402788  hr: 0  min: 0  sec: 8\n",
      "epoch: 5  batch: 1327 / 1463  loss: 0.06500741921634323  hr: 0  min: 0  sec: 8\n",
      "epoch: 5  batch: 1328 / 1463  loss: 0.06540098089797276  hr: 0  min: 0  sec: 8\n",
      "epoch: 5  batch: 1329 / 1463  loss: 0.06539624960513794  hr: 0  min: 0  sec: 8\n",
      "epoch: 5  batch: 1330 / 1463  loss: 0.06572322020544828  hr: 0  min: 0  sec: 8\n",
      "epoch: 5  batch: 1331 / 1463  loss: 0.06672080273968363  hr: 0  min: 0  sec: 8\n",
      "epoch: 5  batch: 1332 / 1463  loss: 0.06731636043296349  hr: 0  min: 0  sec: 8\n",
      "epoch: 5  batch: 1333 / 1463  loss: 0.0672728582614373  hr: 0  min: 0  sec: 8\n",
      "epoch: 5  batch: 1334 / 1463  loss: 0.06739030189635815  hr: 0  min: 0  sec: 8\n",
      "epoch: 5  batch: 1335 / 1463  loss: 0.06734309074151057  hr: 0  min: 0  sec: 8\n",
      "epoch: 5  batch: 1336 / 1463  loss: 0.06729348510896148  hr: 0  min: 0  sec: 8\n",
      "epoch: 5  batch: 1337 / 1463  loss: 0.06724890611440876  hr: 0  min: 0  sec: 8\n",
      "epoch: 5  batch: 1338 / 1463  loss: 0.06720006691692769  hr: 0  min: 0  sec: 8\n",
      "epoch: 5  batch: 1339 / 1463  loss: 0.06715062046930861  hr: 0  min: 0  sec: 8\n",
      "epoch: 5  batch: 1340 / 1463  loss: 0.06710130036693439  hr: 0  min: 0  sec: 7\n",
      "epoch: 5  batch: 1341 / 1463  loss: 0.06705227050374923  hr: 0  min: 0  sec: 7\n",
      "epoch: 5  batch: 1342 / 1463  loss: 0.06702498389513738  hr: 0  min: 0  sec: 7\n",
      "epoch: 5  batch: 1343 / 1463  loss: 0.06698785479324322  hr: 0  min: 0  sec: 7\n",
      "epoch: 5  batch: 1344 / 1463  loss: 0.06723322881266756  hr: 0  min: 0  sec: 7\n",
      "epoch: 5  batch: 1345 / 1463  loss: 0.0672578843946432  hr: 0  min: 0  sec: 7\n",
      "epoch: 5  batch: 1346 / 1463  loss: 0.06756551412683112  hr: 0  min: 0  sec: 7\n",
      "epoch: 5  batch: 1347 / 1463  loss: 0.06755665433945279  hr: 0  min: 0  sec: 7\n",
      "epoch: 5  batch: 1348 / 1463  loss: 0.06767514060953621  hr: 0  min: 0  sec: 7\n",
      "epoch: 5  batch: 1349 / 1463  loss: 0.06763540436154293  hr: 0  min: 0  sec: 7\n",
      "epoch: 5  batch: 1350 / 1463  loss: 0.06763058376246288  hr: 0  min: 0  sec: 7\n",
      "epoch: 5  batch: 1351 / 1463  loss: 0.06760641605445311  hr: 0  min: 0  sec: 7\n",
      "epoch: 5  batch: 1352 / 1463  loss: 0.0675731866010463  hr: 0  min: 0  sec: 7\n",
      "epoch: 5  batch: 1353 / 1463  loss: 0.06753807317204233  hr: 0  min: 0  sec: 7\n",
      "epoch: 5  batch: 1354 / 1463  loss: 0.06752625522928975  hr: 0  min: 0  sec: 7\n",
      "epoch: 5  batch: 1355 / 1463  loss: 0.06747851452995968  hr: 0  min: 0  sec: 6\n",
      "epoch: 5  batch: 1356 / 1463  loss: 0.06745361573955956  hr: 0  min: 0  sec: 6\n",
      "epoch: 5  batch: 1357 / 1463  loss: 0.06774668361330298  hr: 0  min: 0  sec: 6\n",
      "epoch: 5  batch: 1358 / 1463  loss: 0.06786012549535114  hr: 0  min: 0  sec: 6\n",
      "epoch: 5  batch: 1359 / 1463  loss: 0.06782805280251696  hr: 0  min: 0  sec: 6\n",
      "epoch: 5  batch: 1360 / 1463  loss: 0.06777917317237606  hr: 0  min: 0  sec: 6\n",
      "epoch: 5  batch: 1361 / 1463  loss: 0.06774339432308323  hr: 0  min: 0  sec: 6\n",
      "epoch: 5  batch: 1362 / 1463  loss: 0.0677223892629421  hr: 0  min: 0  sec: 6\n",
      "epoch: 5  batch: 1363 / 1463  loss: 0.06790079382956367  hr: 0  min: 0  sec: 6\n",
      "epoch: 5  batch: 1364 / 1463  loss: 0.06786402051932028  hr: 0  min: 0  sec: 6\n",
      "epoch: 5  batch: 1365 / 1463  loss: 0.06797950117456071  hr: 0  min: 0  sec: 6\n",
      "epoch: 5  batch: 1366 / 1463  loss: 0.06793174118465846  hr: 0  min: 0  sec: 6\n",
      "epoch: 5  batch: 1367 / 1463  loss: 0.06788450031600134  hr: 0  min: 0  sec: 6\n",
      "epoch: 5  batch: 1368 / 1463  loss: 0.06784108290162427  hr: 0  min: 0  sec: 6\n",
      "epoch: 5  batch: 1369 / 1463  loss: 0.06791546092109045  hr: 0  min: 0  sec: 6\n",
      "epoch: 5  batch: 1370 / 1463  loss: 0.06786622510204111  hr: 0  min: 0  sec: 5\n",
      "epoch: 5  batch: 1371 / 1463  loss: 0.06803788294368487  hr: 0  min: 0  sec: 5\n",
      "epoch: 5  batch: 1372 / 1463  loss: 0.06801190177500879  hr: 0  min: 0  sec: 5\n",
      "epoch: 5  batch: 1373 / 1463  loss: 0.06796312156084684  hr: 0  min: 0  sec: 5\n",
      "epoch: 5  batch: 1374 / 1463  loss: 0.06791403876930026  hr: 0  min: 0  sec: 5\n",
      "epoch: 5  batch: 1375 / 1463  loss: 0.06786547522729432  hr: 0  min: 0  sec: 5\n",
      "epoch: 5  batch: 1376 / 1463  loss: 0.06783901638871527  hr: 0  min: 0  sec: 5\n",
      "epoch: 5  batch: 1377 / 1463  loss: 0.06779067958120091  hr: 0  min: 0  sec: 5\n",
      "epoch: 5  batch: 1378 / 1463  loss: 0.0677420715070654  hr: 0  min: 0  sec: 5\n",
      "epoch: 5  batch: 1379 / 1463  loss: 0.0677431325641433  hr: 0  min: 0  sec: 5\n",
      "epoch: 5  batch: 1380 / 1463  loss: 0.06769436931413045  hr: 0  min: 0  sec: 5\n",
      "epoch: 5  batch: 1381 / 1463  loss: 0.06772012661085083  hr: 0  min: 0  sec: 5\n",
      "epoch: 5  batch: 1382 / 1463  loss: 0.0676763196878119  hr: 0  min: 0  sec: 5\n",
      "epoch: 5  batch: 1383 / 1463  loss: 0.06763225120060626  hr: 0  min: 0  sec: 5\n",
      "epoch: 5  batch: 1384 / 1463  loss: 0.06788513379479688  hr: 0  min: 0  sec: 5\n",
      "epoch: 5  batch: 1385 / 1463  loss: 0.06826551503498471  hr: 0  min: 0  sec: 5\n",
      "epoch: 5  batch: 1386 / 1463  loss: 0.06823094648008018  hr: 0  min: 0  sec: 4\n",
      "epoch: 5  batch: 1387 / 1463  loss: 0.06858040420096784  hr: 0  min: 0  sec: 4\n",
      "epoch: 5  batch: 1388 / 1463  loss: 0.06853986927744281  hr: 0  min: 0  sec: 4\n",
      "epoch: 5  batch: 1389 / 1463  loss: 0.06849312298288174  hr: 0  min: 0  sec: 4\n",
      "epoch: 5  batch: 1390 / 1463  loss: 0.0684546708903215  hr: 0  min: 0  sec: 4\n",
      "epoch: 5  batch: 1391 / 1463  loss: 0.06844990791841313  hr: 0  min: 0  sec: 4\n",
      "epoch: 5  batch: 1392 / 1463  loss: 0.06840263468972087  hr: 0  min: 0  sec: 4\n",
      "epoch: 5  batch: 1393 / 1463  loss: 0.06835406919378367  hr: 0  min: 0  sec: 4\n",
      "epoch: 5  batch: 1394 / 1463  loss: 0.06857661175150077  hr: 0  min: 0  sec: 4\n",
      "epoch: 5  batch: 1395 / 1463  loss: 0.0685279149622623  hr: 0  min: 0  sec: 4\n",
      "epoch: 5  batch: 1396 / 1463  loss: 0.06848392635854902  hr: 0  min: 0  sec: 4\n",
      "epoch: 5  batch: 1397 / 1463  loss: 0.06843508427046255  hr: 0  min: 0  sec: 4\n",
      "epoch: 5  batch: 1398 / 1463  loss: 0.06838667682080662  hr: 0  min: 0  sec: 4\n",
      "epoch: 5  batch: 1399 / 1463  loss: 0.06841748091136339  hr: 0  min: 0  sec: 4\n",
      "epoch: 5  batch: 1400 / 1463  loss: 0.06837387844947183  hr: 0  min: 0  sec: 4\n",
      "epoch: 5  batch: 1401 / 1463  loss: 0.0683254961723631  hr: 0  min: 0  sec: 3\n",
      "epoch: 5  batch: 1402 / 1463  loss: 0.06827764810018853  hr: 0  min: 0  sec: 3\n",
      "epoch: 5  batch: 1403 / 1463  loss: 0.06826511688848824  hr: 0  min: 0  sec: 3\n",
      "epoch: 5  batch: 1404 / 1463  loss: 0.0682302517989928  hr: 0  min: 0  sec: 3\n",
      "epoch: 5  batch: 1405 / 1463  loss: 0.06818377013640144  hr: 0  min: 0  sec: 3\n",
      "epoch: 5  batch: 1406 / 1463  loss: 0.06813732742863574  hr: 0  min: 0  sec: 3\n",
      "epoch: 5  batch: 1407 / 1463  loss: 0.06810614056634126  hr: 0  min: 0  sec: 3\n",
      "epoch: 5  batch: 1408 / 1463  loss: 0.068058409212447  hr: 0  min: 0  sec: 3\n",
      "epoch: 5  batch: 1409 / 1463  loss: 0.0680148434957479  hr: 0  min: 0  sec: 3\n",
      "epoch: 5  batch: 1410 / 1463  loss: 0.06807180557407005  hr: 0  min: 0  sec: 3\n",
      "epoch: 5  batch: 1411 / 1463  loss: 0.06803519506407832  hr: 0  min: 0  sec: 3\n",
      "epoch: 5  batch: 1412 / 1463  loss: 0.06803411082358435  hr: 0  min: 0  sec: 3\n",
      "epoch: 5  batch: 1413 / 1463  loss: 0.06798662342189551  hr: 0  min: 0  sec: 3\n",
      "epoch: 5  batch: 1414 / 1463  loss: 0.06794032937418712  hr: 0  min: 0  sec: 3\n",
      "epoch: 5  batch: 1415 / 1463  loss: 0.06807950265689543  hr: 0  min: 0  sec: 3\n",
      "epoch: 5  batch: 1416 / 1463  loss: 0.06803206619238898  hr: 0  min: 0  sec: 3\n",
      "epoch: 5  batch: 1417 / 1463  loss: 0.06822950690759207  hr: 0  min: 0  sec: 2\n",
      "epoch: 5  batch: 1418 / 1463  loss: 0.06822360367072094  hr: 0  min: 0  sec: 2\n",
      "epoch: 5  batch: 1419 / 1463  loss: 0.06817571294467392  hr: 0  min: 0  sec: 2\n",
      "epoch: 5  batch: 1420 / 1463  loss: 0.06842373059316485  hr: 0  min: 0  sec: 2\n",
      "epoch: 5  batch: 1421 / 1463  loss: 0.06838666355536555  hr: 0  min: 0  sec: 2\n",
      "epoch: 5  batch: 1422 / 1463  loss: 0.06833913577497025  hr: 0  min: 0  sec: 2\n",
      "epoch: 5  batch: 1423 / 1463  loss: 0.06830257590406412  hr: 0  min: 0  sec: 2\n",
      "epoch: 5  batch: 1424 / 1463  loss: 0.06825568226195991  hr: 0  min: 0  sec: 2\n",
      "epoch: 5  batch: 1425 / 1463  loss: 0.06821621049975715  hr: 0  min: 0  sec: 2\n",
      "epoch: 5  batch: 1426 / 1463  loss: 0.06817286387787182  hr: 0  min: 0  sec: 2\n",
      "epoch: 5  batch: 1427 / 1463  loss: 0.06813390406474996  hr: 0  min: 0  sec: 2\n",
      "epoch: 5  batch: 1428 / 1463  loss: 0.06808830448650495  hr: 0  min: 0  sec: 2\n",
      "epoch: 5  batch: 1429 / 1463  loss: 0.06820590203386641  hr: 0  min: 0  sec: 2\n",
      "epoch: 5  batch: 1430 / 1463  loss: 0.06816088832500455  hr: 0  min: 0  sec: 2\n",
      "epoch: 5  batch: 1431 / 1463  loss: 0.06815770982938267  hr: 0  min: 0  sec: 2\n",
      "epoch: 5  batch: 1432 / 1463  loss: 0.06811049348084834  hr: 0  min: 0  sec: 1\n",
      "epoch: 5  batch: 1433 / 1463  loss: 0.06806347526032126  hr: 0  min: 0  sec: 1\n",
      "epoch: 5  batch: 1434 / 1463  loss: 0.06801744804826709  hr: 0  min: 0  sec: 1\n",
      "epoch: 5  batch: 1435 / 1463  loss: 0.0679704314081537  hr: 0  min: 0  sec: 1\n",
      "epoch: 5  batch: 1436 / 1463  loss: 0.06792341031837484  hr: 0  min: 0  sec: 1\n",
      "epoch: 5  batch: 1437 / 1463  loss: 0.06788312384339273  hr: 0  min: 0  sec: 1\n",
      "epoch: 5  batch: 1438 / 1463  loss: 0.06784354721790864  hr: 0  min: 0  sec: 1\n",
      "epoch: 5  batch: 1439 / 1463  loss: 0.06779933275787264  hr: 0  min: 0  sec: 1\n",
      "epoch: 5  batch: 1440 / 1463  loss: 0.0680623465706756  hr: 0  min: 0  sec: 1\n",
      "epoch: 5  batch: 1441 / 1463  loss: 0.06803203293593829  hr: 0  min: 0  sec: 1\n",
      "epoch: 5  batch: 1442 / 1463  loss: 0.0680124801613459  hr: 0  min: 0  sec: 1\n",
      "epoch: 5  batch: 1443 / 1463  loss: 0.06796595211260534  hr: 0  min: 0  sec: 1\n",
      "epoch: 5  batch: 1444 / 1463  loss: 0.06791956817295591  hr: 0  min: 0  sec: 1\n",
      "epoch: 5  batch: 1445 / 1463  loss: 0.06906098122861572  hr: 0  min: 0  sec: 1\n",
      "epoch: 5  batch: 1446 / 1463  loss: 0.06904625151324746  hr: 0  min: 0  sec: 1\n",
      "epoch: 5  batch: 1447 / 1463  loss: 0.06900227768933974  hr: 0  min: 0  sec: 1\n",
      "epoch: 5  batch: 1448 / 1463  loss: 0.0689597503798763  hr: 0  min: 0  sec: 0\n",
      "epoch: 5  batch: 1449 / 1463  loss: 0.06892607313845296  hr: 0  min: 0  sec: 0\n",
      "epoch: 5  batch: 1450 / 1463  loss: 0.06888188848986515  hr: 0  min: 0  sec: 0\n",
      "epoch: 5  batch: 1451 / 1463  loss: 0.06883693883764519  hr: 0  min: 0  sec: 0\n",
      "epoch: 5  batch: 1452 / 1463  loss: 0.06879725717862788  hr: 0  min: 0  sec: 0\n",
      "epoch: 5  batch: 1453 / 1463  loss: 0.06924537591469851  hr: 0  min: 0  sec: 0\n",
      "epoch: 5  batch: 1454 / 1463  loss: 0.0692105229516921  hr: 0  min: 0  sec: 0\n",
      "epoch: 5  batch: 1455 / 1463  loss: 0.069169654417543  hr: 0  min: 0  sec: 0\n",
      "epoch: 5  batch: 1456 / 1463  loss: 0.06912235811014998  hr: 0  min: 0  sec: 0\n",
      "epoch: 5  batch: 1457 / 1463  loss: 0.06942200568404276  hr: 0  min: 0  sec: 0\n",
      "epoch: 5  batch: 1458 / 1463  loss: 0.06939836356415834  hr: 0  min: 0  sec: 0\n",
      "epoch: 5  batch: 1459 / 1463  loss: 0.06975715466972243  hr: 0  min: 0  sec: 0\n",
      "epoch: 5  batch: 1460 / 1463  loss: 0.07102095706209745  hr: 0  min: 0  sec: 0\n",
      "epoch: 5  batch: 1461 / 1463  loss: 0.07098266690418706  hr: 0  min: 0  sec: 0\n",
      "epoch: 5  batch: 1462 / 1463  loss: 0.07100165280145268  hr: 0  min: 0  sec: 0\n",
      "epoch: 5  batch: 1463 / 1463  loss: 0.07095906901047887  hr: 0  min: 0  sec: 0\n"
     ]
    }
   ],
   "source": [
    "train(train_loader, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# import seaborn as sn\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model('bert_ABSA_2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_c_matrix(c_matrix, title=\"test\"):\n",
    "    aix = []\n",
    "    for y in range(len(c_matrix)):\n",
    "        aix.append(y)\n",
    "    df_cm = pd.DataFrame(c_matrix, aix, aix)\n",
    "    sn.heatmap(df_cm, annot=True, fmt='g')\n",
    "    plt.ylabel(\"prediction\")\n",
    "    plt.xlabel(\"ground truth\")\n",
    "    plt.title(title)\n",
    "    plt.savefig(title+'.jpg')\n",
    "    plt.show()\n",
    "\n",
    "def test(loader):\n",
    "    pred = []\n",
    "    trueth = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "\n",
    "            ids_tensors, segments_tensors, masks_tensors, label_ids = data\n",
    "            ids_tensors = ids_tensors.to(DEVICE)\n",
    "            segments_tensors = segments_tensors.to(DEVICE)\n",
    "            masks_tensors = masks_tensors.to(DEVICE)\n",
    "\n",
    "            outputs = model(ids_tensors, None, masks_tensors=masks_tensors, segments_tensors=segments_tensors)\n",
    "            \n",
    "            _, predictions = torch.max(outputs, dim=1)\n",
    "\n",
    "            pred += list([int(i) for i in predictions])\n",
    "            trueth += list([int(i) for i in label_ids])\n",
    "\n",
    "    return trueth, pred\n",
    "\n",
    "def predict(sentence, aspect, tokenizer):\n",
    "    t1 = tokenizer.tokenize(sentence)\n",
    "    t2 = tokenizer.tokenize(aspect)\n",
    "\n",
    "    word_pieces = ['[cls]']\n",
    "    word_pieces += t1\n",
    "    word_pieces += ['[sep]']\n",
    "    word_pieces += t2\n",
    "\n",
    "    segment_tensor = [0] + [0]*len(t1) + [0] + [1]*len(t2)\n",
    "\n",
    "    ids = tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "    input_tensor = torch.tensor([ids]).to(DEVICE)\n",
    "    segment_tensor = torch.tensor(segment_tensor).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor, None, None, segments_tensors=segment_tensor)\n",
    "        _, predictions = torch.max(outputs, dim=1)\n",
    "    \n",
    "    return word_pieces, predictions, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.84 s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.71      0.72       346\n",
      "           1       0.62      0.68      0.65       355\n",
      "           2       0.89      0.88      0.89      1025\n",
      "\n",
      "    accuracy                           0.80      1726\n",
      "   macro avg       0.75      0.75      0.75      1726\n",
      "weighted avg       0.81      0.80      0.80      1726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%time x, y = test(test_loader)\n",
    "print(classification_report(x, y, target_names=[str(i) for i in range(3)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[cls]', 'not', 'impressed', 'with', 'this', 'movie', ',', 'but', 'the', 'actors', 'performance', 'well', '.', '[sep]', 'movie']\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[ 0.7563,  0.1108, -1.3406]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x, y, z = predict(\"Not impressed with this movie, but the actors performance well.\", \"movie\", tokenizer)\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[cls]', 'not', 'impressed', 'with', 'this', 'movie', ',', 'but', 'the', 'actors', 'performance', 'well', '.', '[sep]', 'actors']\n",
      "tensor([2], device='cuda:0')\n",
      "tensor([[-1.9568, -2.5082,  5.1625]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x, y, z = predict(\"Not impressed with this movie, but the actors performance well.\", \"actors\", tokenizer)\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[cls]', 'for', 'the', 'price', 'you', 'pay', 'this', 'product', 'is', 'very', 'good', '.', 'however', ',', 'battery', 'life', 'is', 'a', 'little', 'lack', '-', 'lust', '##er', 'coming', 'from', 'a', 'mac', '##book', 'pro', '.', '[sep]', 'price']\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([[-2.8797e-01,  3.3837e-01, -1.4844e-04]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x, y, z = predict(\"For the price you pay this product is very good. However, battery life is a little lack-luster coming from a MacBook Pro.\", \"price\", tokenizer)\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[cls]', 'for', 'the', 'price', 'you', 'pay', 'this', 'product', 'is', 'very', 'good', '.', 'however', ',', 'battery', 'life', 'is', 'a', 'little', 'lack', '-', 'lust', '##er', 'coming', 'from', 'a', 'mac', '##book', 'pro', '.', '[sep]', 'battery', 'life']\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[ 2.4636, -1.9068, -0.8480]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x, y, z = predict(\"For the price you pay this product is very good. However, battery life is a little lack-luster coming from a MacBook Pro.\", \"battery life\", tokenizer)\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[cls]', 'python', \"'\", 's', 'design', 'philosophy', 'emphasizes', 'code', 'read', '##ability', 'with', 'its', 'notable', 'use', 'of', 'significant', 'whites', '##pace', '[sep]', 'python']\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([[-0.8861,  0.0493, -0.0957]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x, y, z = predict(\"Python's design philosophy emphasizes code readability with its notable use of significant whitespace\", \"python\", tokenizer)\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[cls]', 'the', 'am', '##d', 'ry', '##zen', 'is', 'really', 'slow', '.', '[sep]', 'am', '##d', 'ry', '##zen']\n",
      "tensor([2], device='cuda:0')\n",
      "tensor([[ 0.5644, -1.6248,  0.6487]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x, y, z = predict(\"The AMD Ryzen is really slow.\", \"AMD Ryzen\", tokenizer)\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[cls]', 'the', 'am', '##d', 'ry', '##zen', 'is', 'really', 'fast', '[sep]', 'am', '##d', 'ry', '##zen']\n",
      "tensor([2], device='cuda:0')\n",
      "tensor([[-2.1341, -2.8694,  5.8294]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x, y, z = predict(\"The AMD Ryzen is really fast\", \"AMD Ryzen\", tokenizer)\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[cls]', 'apple', 'is', 'better', 'than', 'microsoft', '.', '[sep]', 'apple']\n",
      "tensor([2], device='cuda:0')\n",
      "tensor([[-0.6887, -1.6353,  2.0344]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x, y, z = predict(\"Apple is better than Microsoft.\", \"Apple\", tokenizer)\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[cls]', 'apple', 'is', 'better', 'than', 'microsoft', '.', '[sep]', 'microsoft']\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[ 1.0112, -1.4838, -0.0873]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x, y, z = predict(\"Apple is better than Microsoft.\", \"Microsoft\", tokenizer)\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[cls]', 'am', '##d', \"'\", 's', 'cpu', 'is', 'better', 'than', 'intel', \"'\", 's', 'cpu', '[sep]', 'cpu']\n",
      "tensor([2], device='cuda:0')\n",
      "tensor([[-0.3281,  0.1926,  0.5652]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x, y, z = predict(\"AMD's cpu is better than Intel's cpu\", \"cpu\", tokenizer)\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[cls]', 'am', '##d', \"'\", 's', 'cpu', 'is', 'better', 'than', 'intel', \"'\", 's', 'cpu', '[sep]', 'intel']\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[ 0.2396, -0.5333, -0.5908]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x, y, z = predict(\"AMD's cpu is better than Intel's cpu\", \"Intel\", tokenizer)\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[cls]', 'i', 'have', 'to', 'say', 'they', 'have', 'one', 'of', 'the', 'fastest', 'delivery', 'times', 'in', 'the', 'city', '[sep]', 'delivery', 'times']\n",
      "tensor([2], device='cuda:0')\n",
      "tensor([[-2.0156, -2.8365,  5.4197]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x, y, z = predict(\"I have to say they have one of the fastest delivery times in the city\",\"delivery times\", tokenizer)\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[cls]', 'i', 'have', 'to', 'say', 'they', 'have', 'one', 'of', 'the', 'slow', '##est', 'delivery', 'times', 'in', 'the', 'city', '[sep]', 'delivery', 'times']\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[ 2.2668, -1.7621, -0.6383]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x, y, z = predict(\"I have to say they have one of the slowest delivery times in the city\",\"delivery times\", tokenizer)\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[cls]', 'great', 'food', 'but', 'the', 'service', 'was', 'dreadful', '[sep]', 'service']\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[ 2.0604, -1.5593, -0.6599]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x, y, z = predict(\"Great food but the service was dreadful\",\"service\", tokenizer)\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[cls]', 'great', 'food', 'but', 'the', 'service', 'was', 'dreadful', '[sep]', 'food']\n",
      "tensor([2], device='cuda:0')\n",
      "tensor([[-0.9337, -0.6863,  1.9533]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x, y, z = predict(\"Great food but the service was dreadful\",\"food\", tokenizer)\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
